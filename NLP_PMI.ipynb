{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMONo6XtQB5ELJcx6+88za1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#CMT309 – Data Science Portfolio (Spring)\n"],"metadata":{"id":"oJ34uxMILElC"}},{"cell_type":"code","source":["import numpy as np\n","import string\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import math\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# the code block below is directly downloading commentary.txt and superheros.csv into your drive folder. Please just run it and do not comment out.\n","from urllib import request\n","module_url = [f\"https://drive.google.com/uc?export=view&id=18y6hLv2bqAyJsIXwVCty58lF0u7yimVq\"]\n","name = ['commentary.txt']\n","for i in range(len(name)):\n","    with request.urlopen(module_url[i]) as f, open(name[i],'w') as outf:\n","        a = f.read()\n","        outf.write(a.decode('ISO-8859-1'))\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","import nltk\n","import re\n","from tqdm import tqdm\n","tqdm.pandas()\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"id":"7PRe1FadWJR-","executionInfo":{"status":"ok","timestamp":1707136427759,"user_tz":0,"elapsed":1945,"user":{"displayName":"Oktay Karakuş","userId":"07357612204679807063"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c1ebd857-11b7-4300-a501-5392f7060aa9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Q5) Text Analysis (25 marks)\n","\n","In this question, we will interrogate the football commentary dataset"],"metadata":{"id":"gxiHUV1sdEjU"}},{"cell_type":"code","source":["df = pd.read_csv('commentary.txt', sep='\\t')\n","df.head()"],"metadata":{"id":"ej3H78ubdEjV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q5.1 - Preprocessing (2 marks)\n","\n","You must implement a method for obtaining tokenized, PoS-tagged and PoS-tagged and lemmatized versions of the Commentary column. You must use only `nltk` libraries. You must create 3 new columns: `Tokenized`, `PoS_tagged` and `PoS_lemmatized`, and create them in order:\n","\n","1.- New `Tokenized` column, by lower casing and tokenizing the `Commentary` column.\n","\n","2.- New `PoS_tagged` column, by pos_tagging the `Tokenized` column.\n","\n","3.- New `PoS_lemmatized` column, by lemmatizing only the words in the `PoS_tagged` column. The reason for doing it in this order is to present to the tagging function the original text.\n","\n","An example outcome of the returned data frame is given below for each columns first three rows:\n","\n","```python\n",">>print(df['Tokenized'][:3])\n","0    [plenty, of, chances, in, this, game, but, nei...\n","1    [that, 's, it, !, the, referee, blows, the, fi...\n","2    [ball, possession, :, tottenham, :, 44, %, ,, ...\n","Name: Tokenized, dtype: object\n","\n",">>print(df['PoS_tagged'][:3])\n","0    [(plenty, NN), (of, IN), (chances, NNS), (in, ...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_tagged, dtype: object\n","\n",">>print(df['PoS_lemmatized'][:3])\n","0    [(plenty, NN), (of, IN), (chance, NNS), (in, I...\n","1    [(that, DT), ('s, VBZ), (it, PRP), (!, .), (th...\n","2    [(ball, DT), (possession, NN), (:, :), (totten...\n","Name: PoS_lemmatized, dtype: object\n","```"],"metadata":{"id":"lM-Hl3i69JrI"}},{"cell_type":"code","source":["# Q5.1 - Your code here"],"metadata":{"id":"Ix6Yi0c_P7E9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q5.2 - Basic search engine (10 marks)\n","\n","In this question, we implement a basic search engine in a function called `retrieve_similar_commentaries(df, query, k)`, which takes as input the following arguments:\n","\n","- `df` the previously enriched (tokenized, pos tagged, etc) commentary dataframe.\n","- `query` a string of any type, which will be the query we will be using to retrieve similar commentaries.\n","- `k` and integer denoting the top `k` commentaries to be returned (by similarity).\n","\n","Our function must perform the following steps:\n","\n","1 - Tokenize and lemmatize the input query.\n","\n","2 - For each commentary in the df, compute how similar it is to the query as the number of shared tokens between query and commentary.\n","\n","3 - We will prioritize noun matches, so our similarity score will receive +1 if at least one of the matching tokens in the commentary is a noun (i.e., its part of speech starts with `N`). This means that, for example, if your query has 2 tokens, the maximum similarity a commentary can have is 4: 2 for 2 overlapping tokens, and 2 for both tokens being nouns.\n","\n","4 - The function must return a list of tuples of the form `[(commentary1, sim), (commentary2, sim) ... (commentaryk, sim)]`, where commentaries are ranked by `sim` value in descending order.\n","\n","An example test case is given below:\n","\n","```python\n",">>> result = retrieve_similar_commentaries(df, \"Manchester United ball\", 3)\n",">>> for idx,r in enumerate(result):\n",">>>   print(idx,r)\n","\n","0 ('Manchester United is in control of the ball.', 5)\n","1 ('Manchester United is in control of the ball.', 5)\n","2 ('Jadon Sancho from Manchester United crosses the ball, but it goes out for a goal kick.', 5)\n","```"],"metadata":{"id":"IYsMTNgJWO-t"}},{"cell_type":"code","source":["def retrieve_similar_commentaries(df, query, k):\n","    # your code here\n","    pass"],"metadata":{"id":"QTj15d4bQSl0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q5.3 - PMI (13 marks)\n","\n","In this question, you implement and apply the pointwise mutual information (PMI) metric, a word association metric introduced in 1992, to the football commentaries. The purpose of PMI is to extract, from free text, pairs of words or phrases than tend to co-occur together more often than expected by chance. For example, PMI(`new`, `york`) would give a higher score than PMI(`new`, `car`) because the chance of finding `new` and `york` together in text is higher than `new` and `car`, despite `new` being a more frequent word than `york`.\n","\n","The formula for PMI (where `x` and `y` are two words) is:\n","\n","$PMI(x,y) = log(\\frac{p(x,y)}{p(x)p(y)})$\n","\n","Watch this video to understand how to estimate these probabilities: https://www.youtube.com/watch?v=swDoFpuHpzQ."],"metadata":{"id":"xdbA8XlR9j6P"}},{"cell_type":"markdown","source":["Detailed instructions:\n","\n","You will implement the following logic:\n","\n","- **Phrase Extraction**: The first step is to extract noun phrases (NPs) and verb phrases (VPs) from the lemmatized data. To do this, you'll need to write a function that goes through each entry and groups words into noun phrases or verb phrases based on their part-of-speech tags. We will reward cases where NPs and VPs go beyond single word matching.\n","\n","- **Phrase Counting**: Once you have extracted the NPs and VPs, you'll need to count how many times each phrase occurs in the dataset. You'll have to write a function that iterates through the NPs and VPs and keeps track of the counts in dictionaries.\n","\n","- **Total Counts**: The next step is to compute the total count of all NPs and VPs. This is simply the sum of all the counts in the dictionaries you created in the previous step.\n","\n","- **Identifying Top Phrases**: To reduce computational complexity, we only want to compute PMI for the top occurring NPs and VPs. So, you will need to write a function that sorts the phrases by their counts and selects the top 100 phrases.\n","\n","- **Creating the PMI Matrix**: Finally, you'll create a PMI matrix using the top NPs and VPs, their counts, and the total counts of NPs and VPs. This matrix will be a pandas DataFrame, which will have rows corresponding to the top VPs, columns corresponding to the top NPs, and each cell will contain the PMI value between the corresponding NP and VP. This part of your solution will return 0 when there is no co-occurrence between an NP and a VP, and apply smoothing only to the final PMI value (refer to the video).\n","\n","You must implement all the functionality in a function called `compute_pmi_dataframe(df)` that takes as input the enriched `df` you created in `Q5.1`. You are encouraged to implement additional functions to break down your code and make it clear how you are separating different functionalities.\n","\n","An example test case is given below:\n","\n","```python\n",">>> def top_k_vps(pmi_matrix, np, k):\n",">>>    # Check if the NP exists in the matrix\n",">>>    if np in pmi_matrix.T.index:\n",">>>        top_vps = pmi_matrix.T.loc[np].nlargest(k)\n",">>>        return top_vps.index.tolist()\n",">>>    else:\n",">>>        print(f\"Noun phrase '{np}' not found in PMI matrix.\")\n",">>>        return []\n",">>>top_k_vps(pmidf, 'joao cancelo', 3)\n","\n","['arrives', 'cut', 'benefit']\n","```"],"metadata":{"id":"zjwmwOLHMyB9"}},{"cell_type":"code","source":["def compute_pmi_dataframe(df):\n","    # 1 - PHRASE EXTRACTION\n","    # your code here\n","\n","    # 2 - PHRASE COUNTING\n","    # your code here\n","\n","    # 3 - TOTAL COUNTS\n","    # your code here\n","\n","    # 4 - FIND TOP PHRASES\n","    # your code here\n","\n","    # 5 - CREATE PMI MATRIX\n","    # your code here\n","    return pmi_matrix\n","\n","pmidf = compute_pmi_dataframe(df)"],"metadata":{"id":"nwjShxK2RpGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you can test your resulting matrix\n","def top_k_vps(pmi_matrix, np, k):\n","    # Check if the NP exists in the matrix\n","    if np in pmi_matrix.T.index:\n","        top_vps = pmi_matrix.T.loc[np].nlargest(k)\n","        return top_vps.index.tolist()\n","    else:\n","        print(f\"Noun phrase '{np}' not found in PMI matrix.\")\n","        return []\n","top_k_vps(pmidf, 'joao cancelo', 3)"],"metadata":{"id":"fo9ahEkyJlH2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707136567731,"user_tz":0,"elapsed":245,"user":{"displayName":"Oktay Karakuş","userId":"07357612204679807063"}},"outputId":"209aca0c-36c7-4fac-ddf0-2840a9385105"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['arrives', 'cut', 'benefit']"]},"metadata":{},"execution_count":13}]}]}